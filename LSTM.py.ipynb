{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data, Remove empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentiment_dataset_train.csv\")\n",
    "df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].value_counts()  # all classes are distributed almost equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of <b>clean</b> function:\n",
    "<br>- Create a new column of clean reviews so, it is easy to train</br>\n",
    "<br>- Convert revires in lower case</br>\n",
    "<br>- Remove special charectors</br>\n",
    "\n",
    "<br>- There are very few reviews which contains emojis in text so, we do not need to remove it.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(Data_frame):\n",
    "    Data_frame['Clean_data'] = np.nan*len(Data_frame['review'])     # Create an empty column for clean data\n",
    "    for i in range(len(Data_frame['review'])):\n",
    "        print(i)\n",
    "        review = Data_frame['review'][i]\n",
    "        #print(review)\n",
    "        review_lower_case = review.lower()    # convert reviews into lower case\n",
    "        remove_special_charactors = ''.join([c for c in review_lower_case if c not in punctuation])  # remove special charactor\n",
    "        reviews_split = remove_special_charactors.split('\\n')\n",
    "        #filtered_sentence = ' '.join([word for word in str(reviews_split).split() if word not in stopwords.words(\"english\")])\n",
    "        print(reviews_split)\n",
    "        #Data_frame.loc[i,['Clean_data']]=reviews_split  # adding clean review to our new column\n",
    "        Data_frame['Clean_data'][i]=str(reviews_split)\n",
    "    return Data_frame['Clean_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # chack that new clean data column has addeded to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of prepare_data function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>- Create vocabulary (Bag of words)</br>\n",
    "<br>- Convert words into integers (Tokenize)</br>\n",
    "<br>- Converts labels into categories with keras to_categorical function</br>\n",
    "<br>- Identify the average length of revies so we can remove very short and big reviews.</br>\n",
    "<br>- Convert clean reviews into integers and pad the reviews with average length of reviews.</br>\n",
    "<br>- Split the created data into train and validation.</br>\n",
    "<br>- Return ready to go data in LSTM</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(Reviews, Label, seq_length, train_test_split, training):\n",
    "    \n",
    "    #Reviews- New column of clean data\n",
    "    #Label - Rating column of data\n",
    "    #seq_length - average length of individual review\n",
    "    #train_test_split - Fraction of data that we want to train and validate: i.e: 0.8  means 80% data in training and 20% in validation\n",
    "    #training - True , if you want to use the function for training purpose.\n",
    "    all_text = ' '.join(review for review in Reviews) \n",
    "    words = all_text.split()    \n",
    "    count_words = Counter(words) \n",
    "    total_words = len(words)\n",
    "    \n",
    "    vocab_to_int = {w:i for i, w in enumerate(count_words)} # Create dictionary for words and convert them into integer\n",
    "    print('length of vocab:', len(vocab_to_int))\n",
    "    \n",
    "    reviews_int = []\n",
    "    for i in range(len(Reviews)):\n",
    "        review = Reviews[i]\n",
    "        r = [vocab_to_int[w] for w in review.split()]\n",
    "        reviews_int.append(r)\n",
    "        \n",
    "    label = to_categorical(Label) #Convert labels into categories\n",
    "    \n",
    "    #plot\n",
    "    reviews_len = [len(x) for x in reviews_int]\n",
    "    pd.Series(reviews_len).hist()\n",
    "    plt.legend('Average length of reviews')\n",
    "    plt.show()\n",
    "    pd.Series(reviews_len).describe()\n",
    "    \n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    #Padding of reviews\n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "        \n",
    "    Features = features\n",
    "    if training == True:\n",
    "    #Split the data into train and validation set\n",
    "        split_frac =train_test_split\n",
    "        train_x = Features[0:int(split_frac*len(Features))]\n",
    "        train_y = label[0:int(split_frac*len(Features))]\n",
    "        print('Total train reviews:', len(train_x))\n",
    "        print('Total train labels:', len(train_y))\n",
    "    \n",
    "        valid_x = Features[int(split_frac*len(Features)):]\n",
    "        valid_y = label[int(split_frac*len(Features)):]\n",
    "        \n",
    "    else:\n",
    "        train_x = Features\n",
    "        train_y = 0\n",
    "        print('Total train reviews:', len(train_x))\n",
    "        print(train_y)\n",
    "        valid_x = 0\n",
    "        valid_y = 0\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y,  valid_x, valid_y =prepare_data(Reviews, df['rating'], 220, train_test_split=0.8, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(58649, 64)) #The embedding layer\n",
    "model1.add(layers.LSTM(20,dropout=0.2, return_sequences=True)) #Our LSTM layer\n",
    "#model1.add(layers.LSTM(20,dropout=0.2, return_sequences=True))\n",
    "model1.add(layers.LSTM(20,dropout=0.2))\n",
    "model1.add(layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#checkpoint1 = ModelCheckpoint(\"best_model1_1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "model1.fit(train_x, train_y, epochs=30,validation_data=(valid_x, valid_y),batch_size = 32)\n",
    "model1.save('LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('sentiment_dataset_test.csv')  #load test data\n",
    "\n",
    "clean_review = clean(df_1)  # clean test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data =prepare_data(clean_review, Label=0, seq_length = 220, train_test_split =1, training=False)\n",
    "#prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('LSTM')  #load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probabilities = model.predict(prepare_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Predicted_review'] = np.nan*len(df_1['review']) #predict review and save it in new column\n",
    "for i in range(len(class_probabilities)):\n",
    "    array = class_probabilities[i]\n",
    "    prediction = np.argmax(array)\n",
    "    print(prediction)\n",
    "    df_1['Predicted_review'][i] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head() # check new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv('test_result.csv') # save result as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Multiple algorithms</b><br>\n",
    "<li>Hitting at the right machine learning algorithm is the ideal approach to achieve higher accuracy. But, it is easier said than done.</li>\n",
    "\n",
    "<li>This intuition comes with experience and incessant practice. Some algorithms are better suited to a particular type of data sets than others. Hence, we should apply all relevant models and check the performance.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Algorithm Tuning</b><br>\n",
    "<li>We know that machine learning algorithms are driven by parameters. These parameters majorly influence the outcome of learning process.\n",
    "\n",
    "<li>The objective of parameter tuning is to find the optimum value for each parameter to improve the accuracy of the model. To tune these parameters, you must have a good understanding of these meaning and their individual impact on model. You can repeat this process with a number of well performing models.</li>\n",
    "\n",
    "<li>For example: In random forest, we have various parameters like max_features, number_trees, random_state, oob_score and others. Intuitive optimization of these parameter values will result in better and more accurate models.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The workflow can be broken down into following basic steps:</b>\n",
    "<li>Training a machine learning model on a local system.</li>\n",
    "<li>Wrapping the inference logic into a flask application.</li>\n",
    "<li>Using docker to containerize the flask application.</li>\n",
    "<li>Hosting the docker container on an AWS ec2 instance and consuming the web-service.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training a machine learning model on a local system</b><br>\n",
    "<li>The model file generated after training is stored as a pickle file which is a serialized format for storing objects. (In the repo, the file is named ‘trained_model.pkl’)</li>\n",
    "<li>The inference call (.predict()) call requires 4 features per test sample in the form of a numpy array.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Wrapping the inference logic into a flask web service</b><br>\n",
    "<li>Convert local code into functions and build RestAPI</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using docker to containerize the flask application.</b><br>\n",
    "<li>Up to this point, we have a web-service that runs locally. Our ultimate intention is to be able to run this piece of code on a cloud virtual machine.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
